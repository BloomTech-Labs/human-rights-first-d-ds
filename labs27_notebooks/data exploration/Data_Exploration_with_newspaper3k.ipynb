{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3DoGf2g-isd","outputId":"f9af8063-cf2c-4fd6-fd9b-ce9343c6be3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: praw in /usr/local/lib/python3.6/dist-packages (7.1.0)\n","Requirement already satisfied: prawcore<2.0,>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from praw) (1.5.0)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.6/dist-packages (from praw) (0.57.0)\n","Requirement already satisfied: update-checker>=0.17 in /usr/local/lib/python3.6/dist-packages (from praw) (0.18.0)\n","Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from prawcore<2.0,>=1.3.0->praw) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.3.0->praw) (1.24.3)\n","Requirement already satisfied: newspaper3k in /usr/local/lib/python3.6/dist-packages (0.2.8)\n","Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.2.3)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n","Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.0.4)\n","Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.35.1)\n","Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n","Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (0.3)\n","Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (6.0.1)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n","Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (1.1.0)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n","Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.6/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n"]}],"source":"# install PRAW and newspaper3k\n!pip install praw\n!pip3 install newspaper3k"},{"cell_type":"code","execution_count":2,"metadata":{"id":"nC5g2McBACIs"},"outputs":[],"source":"# All imports\nimport pandas as pd\nimport praw\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport pickle\nfrom newspaper import Article\nimport spacy\nfrom collections import Counter\nfrom datetime import datetime"},{"cell_type":"code","execution_count":3,"metadata":{"id":"JFBhzgU0AGwA"},"outputs":[],"source":"# Fix geolocation dataframe by lower all case\ndef lowerify(text):\n    return text.lower()\n\n# Set up cities/states locations datafrane\nlocs_path = 'https://raw.githubusercontent.com/Lambda-School-Labs/Labs25-Human_Rights_First-TeamB-DS/main/project/cities_states.csv'\nlocs_df = pd.read_csv(locs_path)\nlocs_df = locs_df.drop(columns=['Unnamed: 0', 'country'])\nlocs_df['city_ascii'] = locs_df['city_ascii'].apply(lowerify)\nlocs_df['admin_name'] = locs_df['admin_name'].apply(lowerify)\n\n# State to city lookup table\n# Mapping output values list \nstates_map = {}\nfor state in list(locs_df.admin_name.unique()):\n    states_map[state] = locs_df[locs_df['admin_name'] == state]['city_ascii'].to_list()\n\n# Police brutality indentifying nlp\nmodel_file = open('model.pkl', 'rb')\npipeline = pickle.load(model_file)\nmodel_file.close()\n\n# Spacy nlp model\nnlp = spacy.load('en_core_web_sm')\n\n# Set up PRAW\n# PRAW credentials go here\nreddit = praw.Reddit(client_id=CLIENT_ID, client_secret=CLIENT_SECRET, password=PASSWORD, user_agent=USER_AGENT, username=USERNAME)"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XasAAbgrIMg2","outputId":"37d6e5cc-8750-445f-f2db-8044416bd419"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pulling data from Reddit...\n","Number of entries initially pulled: 428\n","\n"]}],"source":"# Grab data from reddit\ndata = []\nprint('Pulling data from Reddit...')\nfor submission in reddit.subreddit(\"news\").top('week', limit=500):\n    data.append([\n        submission.id, submission.title, submission.url\n    ])\n\n# Construct a dataframe with the data, print number of posts created\ncol_names = ['id', 'title', 'url']\ndf = pd.DataFrame(data, columns=col_names)\nprint(f'Number of entries initially pulled: {df.shape[0]}\\n')"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sotATzlmIQ-K","outputId":"ee5158b3-7fec-4de1-a10e-3eb950228862"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting data via newspaper3k...\n","Number of entries with missing data:\n","id         0\n","title      0\n","url        0\n","text      17\n","date     151\n","dtype: int64 \n","\n"]}],"source":"# Pull the text from each article itself using newspaper3k\ncontent_list = []\ndate_list = []\n\n# Use newspaper3k to extract data\nprint('Extracting data via newspaper3k...')\nfor id_url in df['url']:\n    article = Article(id_url)\n    article.download()\n    # If the article doesn't download, the error is thrown in parse()\n    try:\n        article.parse()\n    except:\n        # Add null values to show no connection\n        content_list.append(None)\n        date_list.append(None)\n        continue\n    content_list.append(article.text)\n    date_list.append(article.publish_date)\ndf['text'] = content_list\ndf['date'] = date_list\nprint('Number of entries with missing data:')\nprint(df.isnull().sum(),'\\n')"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ES2Q7ZWxIhhj","outputId":"245d3676-382a-42d3-d813-613623f5c7d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Resulting entry count: 277\n","\n","Filtering through police brutality filter...\n"]}],"source":"# Drop any articles with missing data columns\ndf = df.dropna()\ndf = df.reset_index()\ndf = df.drop(columns='index')\nprint(f'Resulting entry count: {df.shape[0]}\\n')\n\n# Convert date column to pandas timestamps\ndef timestampify(date):\n    return pd.Timestamp(date, unit='s').isoformat()\ndf['date'] = df['date'].apply(timestampify)\n\nprint('Filtering through police brutality filter...')"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jpYA0I7IoZx","outputId":"4be5aa73-a1a5-48e7-8bca-ad101c826e4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries determined to be about police brutality: 17\n"]}],"source":"# Use NLP model to filter posts\ndf['is_police_brutality'] = pipeline.predict(df['title'])\ndf = df[df['is_police_brutality'] == 1]\ndf = df.drop(columns='is_police_brutality')\nprint(f'Number of entries determined to be about police brutality: {df.shape[0]}')"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xF-l2WQ8IqOq","outputId":"8b856863-9563-4136-c0c6-4861d6444c3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing through spaCy...\n"]}],"source":"# Use spaCy to extract location tokens\ntokens_list = []\nprint('Tokenizing through spaCy...')\nfor text in df['text']:\n    doc = nlp(text)\n    ents = [e.text.lower() for e in doc.ents if e.label_ == 'GPE']\n    tokens_list.append(ents)\ndf['tokens'] = tokens_list"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61c1yndWC08z","outputId":"19e1cc91-1ff4-4903-a3c5-3d36a93b4e78"},"outputs":[{"name":"stdout","output_type":"stream","text":["Compiling geolocation data...\n"]}],"source":"# Figure out which city and state the article takes place in\ncity_list = []\nstate_list = []\ngeo_list = []\nprint('Compiling geolocation data...')\nfor tokens in df['tokens']:\n    c = Counter(tokens)\n\n    geo_entry = {'lat': None, 'long': None}\n\n    state_counts = {}\n    for state in states_map:\n        if c[state] > 0:\n            state_counts[state] = c[state]\n\n    max_count = 0\n    max_state = None\n\n    for state in state_counts:\n        if state_counts[state] > max_count:\n            max_count = state_counts[state]\n            max_state = {state: {}}\n        elif state_counts[state] == max_count:\n            max_state[state] = {}\n\n    # If no state is found\n    if max_state is None:\n        city_list.append(None)\n        state_list.append(None)\n        geo_list.append(geo_entry)\n        continue\n\n    max_city = None\n    for state in max_state:  \n        city_counts = {}\n        for city in states_map[state]:\n            if c[city] > 0:\n                city_counts[city] = c[city]\n        max_state[state] = city_counts\n\n        max_count = 0\n        for city in city_counts:\n            if city_counts[city] > max_count:\n                max_count = city_counts[city]\n                max_city = (city, state)\n\n    # If no city is found\n    if max_city is None:\n        city_list.append(None)\n        state_list.append(None)\n        geo_list.append(geo_entry)\n        continue\n\n    # Append city and state to geolocation data\n    city_list.append(max_city[0].title())\n    state_list.append(max_city[1].title())\n    # Now get the geolocation data\n    row = locs_df[(\n        (locs_df['city_ascii'] == max_city[0]) &\n        (locs_df['admin_name'] == max_city[1])\n    )]\n    row = row.reset_index()\n    if row.empty:\n        pass\n    else:\n        geo_entry['lat'] = row['lat'][0]\n        geo_entry['long'] = row['lng'][0]\n    geo_list.append(geo_entry)\n\n"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"tUl01CUvI5yh","outputId":"ae131869-45dd-4c6d-9d22-0793e516819e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries where geolocation data could not be found:\n","id            0\n","title         0\n","url           0\n","text          0\n","date          0\n","tokens        0\n","city         13\n","state        13\n","geocoding     0\n","dtype: int64 \n","\n","Final number of entries: 4\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>state</th>\n","      <th>city</th>\n","      <th>date</th>\n","      <th>title</th>\n","      <th>description</th>\n","      <th>links</th>\n","      <th>geocoding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>j3d4ia</td>\n","      <td>Oregon</td>\n","      <td>Portland</td>\n","      <td>2020-10-01T00:00:00</td>\n","      <td>US Justice Department cracks down on Portland ...</td>\n","      <td>At the end of August, in a green leafy park in...</td>\n","      <td>[https://www.opb.org/article/2020/10/01/us-jus...</td>\n","      <td>{'lat': 45.5371, 'long': -122.65}</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>j2c6rw</td>\n","      <td>California</td>\n","      <td>San Jose</td>\n","      <td>2020-09-30T00:42:00+00:00</td>\n","      <td>San Jose officer facing assault charge in viol...</td>\n","      <td>A San Jose, California, police officer is faci...</td>\n","      <td>[https://www.nbcnews.com/news/us-news/san-jose...</td>\n","      <td>{'lat': 37.3021, 'long': -121.8489}</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>j2qr2c</td>\n","      <td>California</td>\n","      <td>San Jose</td>\n","      <td>2020-09-30T13:19:54+00:00</td>\n","      <td>San Jose Officer Facing Assault Charge in Viol...</td>\n","      <td>A San Jose, California, police officer is faci...</td>\n","      <td>[https://www.nbcboston.com/news/national-inter...</td>\n","      <td>{'lat': 37.3021, 'long': -121.8489}</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>j4e5ys</td>\n","      <td>California</td>\n","      <td>San Francisco</td>\n","      <td>2020-10-03T05:19:51+00:00</td>\n","      <td>Better weather won't keep California from grim...</td>\n","      <td>SAN FRANCISCO (AP) â€” Red flag warnings of extr...</td>\n","      <td>[https://apnews.com/article/wildfires-fires-ca...</td>\n","      <td>{'lat': 37.7562, 'long': -122.443}</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  ...                            geocoding\n","0  j3d4ia  ...    {'lat': 45.5371, 'long': -122.65}\n","1  j2c6rw  ...  {'lat': 37.3021, 'long': -121.8489}\n","2  j2qr2c  ...  {'lat': 37.3021, 'long': -121.8489}\n","3  j4e5ys  ...   {'lat': 37.7562, 'long': -122.443}\n","\n","[4 rows x 8 columns]"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":"# Loop ends, add cities and states onto dataframe\ndf['city'] = city_list\ndf['state'] = state_list\ndf['geocoding'] = geo_list\nprint('Number of entries where geolocation data could not be found:')\nprint(df.isnull().sum(),'\\n')\n\n# Drop any columns with null entries for location\ndf = df.dropna()\ndf = df.reset_index()\ndf = df.drop(columns='index')\n\n# Cleanup to match 846 api\ndef listify(text):\n    return [text]\ndf['links'] = df['url'].apply(listify)\ndf['description'] = df['text']\ndf = df.drop(columns=['tokens', 'text'])\ndf = df[[\n    'id', 'state', 'city',\n    'date', 'title', 'description',\n    'links', 'geocoding'\n]]\n\nprint(f'Final number of entries: {df.shape[0]}')\ndf.head()"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}